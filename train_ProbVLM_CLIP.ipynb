{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2986807e-7850-404f-a957-eaeb16371d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:Pytorch pre-release version 2.1.0a0+32f93b1 - assuming intent to test it\n",
      "/workspace/Inter-CLIP/Inter-ProbVLM/ProbVLM/src/train_probVLM.py:26: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  dtype=torch.cuda.FloatTensor(),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from os.path import join as ospj\n",
    "from os.path import expanduser\n",
    "from munch import Munch as mch\n",
    "import numpy as np\n",
    "\n",
    "from ProbVLM.src.ds import prepare_coco_dataloaders, prepare_flickr_dataloaders, prepare_cub_dataloaders, prepare_flo_dataloaders\n",
    "\n",
    "from utils import *\n",
    "from ProbVLM.src.networks import *\n",
    "from ProbVLM.src.train_probVLM import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdb7f39-293e-48fe-bd3b-da616157f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "length of dataset:  566435\n",
      "Loading COCO Caption: n_images 113287 n_captions 566435...\n",
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "length of dataset:  5000\n",
      "Loading COCO Caption: n_images 1000 n_captions 5000...\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "length of dataset:  25000\n",
      "Loading COCO Caption: n_images 5000 n_captions 25000...\n"
     ]
    }
   ],
   "source": [
    "dataset = 'coco' # coco or flickr\n",
    "data_dir = ospj('dataset', dataset) # e.g. ospj(expanduser('~'), 'Documents', 'jm', 'data', dataset)\n",
    "dataloader_config = mch({\n",
    "    'batch_size': 32,\n",
    "    'random_erasing_prob': 0.,\n",
    "    'traindata_shuffle': True\n",
    "})\n",
    "loaders = load_data_loader(dataset, data_dir, dataloader_config, num_workers=12)\n",
    "coco_train_loader, coco_valid_loader, coco_test_loader = loaders['train'], loaders['val'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_batch = next(iter(coco_test_loader))\n",
    "print(test_batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da29997-5483-4fa0-b927-74b38dc36cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 4 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "# clip_net = load_model('cuda')\n",
    "CLIP_Net = load_model(device='cuda', model_path=None)\n",
    "ProbVLM_Net = BayesCap_for_CLIP(\n",
    "    inp_dim=512,\n",
    "    out_dim=512,\n",
    "    hid_dim=256,\n",
    "    num_layers=3,\n",
    "    p_drop=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_image = torch.rand(16, 3, 224, 224).cuda()\n",
    "# test_text = torch.randint(0, 10000, (16, 77)).cuda()\n",
    "# print(test_image, test_text)\n",
    "# # load clip model\n",
    "# clip_model = clip.load('ViT-B/32')[0].cuda()\n",
    "# clip_model.eval()\n",
    "\n",
    "\n",
    "# xfI, xfT = clip_model(test_image, test_text)\n",
    "# print(xfI.shape, xfT.shape)\n",
    "\n",
    "# image_features = clip_model.encode_image(test_image)\n",
    "# text_features = clip_model.encode_text(test_text)\n",
    "# print(image_features.shape, text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1513f40-28d1-4d4d-8d9b-e113c8cd4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17702 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 17702/17702 [07:19<00:00, 40.25batch/s, loss=0.051] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.08370770773321218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 31.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 0.000763863674364984 | Avg. MAE: 0.025264715775847435\n",
      "current score: 0.025264715775847435 | Last best score: 100000000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 17702/17702 [07:20<00:00, 40.21batch/s, loss=0.0422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.04691280128120528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 17702/17702 [07:25<00:00, 39.71batch/s, loss=0.0377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.03936561965964942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 17702/17702 [07:23<00:00, 39.92batch/s, loss=0.0381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.03768271049807643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 17702/17702 [07:20<00:00, 40.18batch/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.03711980050100356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 17702/17702 [07:21<00:00, 40.06batch/s, loss=0.0365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.03681028306394994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 31.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 4.512842133408412e-05 | Avg. MAE: 0.007152955513447523\n",
      "current score: 0.007152955513447523 | Last best score: 0.025264715775847435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 17702/17702 [07:17<00:00, 40.42batch/s, loss=0.0367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.036601428403598535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 17702/17702 [07:21<00:00, 40.09batch/s, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.02453120628748741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 17702/17702 [07:20<00:00, 40.20batch/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.020301449727302804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 17702/17702 [07:20<00:00, 40.16batch/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.020193297914199344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 17702/17702 [07:20<00:00, 40.20batch/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.02009842294301876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 32.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.867980922223069e-05 | Avg. MAE: 0.006686447188258171\n",
      "current score: 0.006686447188258171 | Last best score: 0.007152955513447523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 17702/17702 [07:23<00:00, 39.87batch/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.02002933864406934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 17702/17702 [07:22<00:00, 39.97batch/s, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01997590216975009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 17702/17702 [07:22<00:00, 39.98batch/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019928444616004966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 17702/17702 [07:22<00:00, 40.05batch/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01988368348301942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 17702/17702 [07:20<00:00, 40.17batch/s, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01984393879465667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 32.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.189422932337038e-05 | Avg. MAE: 0.006003163289278746\n",
      "current score: 0.006003163289278746 | Last best score: 0.006686447188258171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 17702/17702 [07:20<00:00, 40.21batch/s, loss=0.02]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019802878856483845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 17702/17702 [07:21<00:00, 40.06batch/s, loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01976127128148669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 17702/17702 [07:21<00:00, 40.07batch/s, loss=0.0219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01972268820480278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 17702/17702 [07:21<00:00, 40.07batch/s, loss=0.0204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019693310436717015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 17702/17702 [07:22<00:00, 40.01batch/s, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019658968174677927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 32.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.2797426683828235e-05 | Avg. MAE: 0.005997189786285162\n",
      "current score: 0.005997189786285162 | Last best score: 0.006003163289278746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 17702/17702 [07:21<00:00, 40.09batch/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019628957407966757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 17702/17702 [07:24<00:00, 39.86batch/s, loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019603067641056515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 17702/17702 [07:23<00:00, 39.94batch/s, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019586647043549472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 17702/17702 [07:16<00:00, 40.52batch/s, loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01956192418889142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 17702/17702 [07:19<00:00, 40.31batch/s, loss=0.0192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019541655472885812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 33.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.498520527500659e-05 | Avg. MAE: 0.0061958362348377705\n",
      "current score: 0.0061958362348377705 | Last best score: 0.005997189786285162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 17702/17702 [07:21<00:00, 40.10batch/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019526138686425686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 17702/17702 [07:22<00:00, 39.97batch/s, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.0195081277551878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 17702/17702 [07:19<00:00, 40.29batch/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019486635448622603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 17702/17702 [07:22<00:00, 40.00batch/s, loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01946653120969732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 17702/17702 [07:23<00:00, 39.87batch/s, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01945535877294213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 32.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.5646404285216704e-05 | Avg. MAE: 0.006308114156126976\n",
      "current score: 0.006308114156126976 | Last best score: 0.005997189786285162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 17702/17702 [07:20<00:00, 40.15batch/s, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019445397942452702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 17702/17702 [07:21<00:00, 40.12batch/s, loss=0.0191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01943158663753742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 17702/17702 [07:20<00:00, 40.16batch/s, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01941693123060052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 17702/17702 [07:21<00:00, 40.06batch/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019406346426928445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 17702/17702 [07:22<00:00, 39.98batch/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01940121818087644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 32.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.759772516787052e-05 | Avg. MAE: 0.006424692925065756\n",
      "current score: 0.006424692925065756 | Last best score: 0.005997189786285162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 17702/17702 [07:19<00:00, 40.28batch/s, loss=0.0188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019388518142978487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 17702/17702 [07:19<00:00, 40.24batch/s, loss=0.0197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019377940668039585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 17702/17702 [07:20<00:00, 40.17batch/s, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01936812460102219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 17702/17702 [07:20<00:00, 40.20batch/s, loss=0.0191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019357353468123478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 17702/17702 [07:20<00:00, 40.23batch/s, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019348958341034204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 32.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.5763048799708486e-05 | Avg. MAE: 0.006154984701424837\n",
      "current score: 0.006154984701424837 | Last best score: 0.005997189786285162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 17702/17702 [07:21<00:00, 40.13batch/s, loss=0.0191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019341973845875677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 17702/17702 [07:21<00:00, 40.10batch/s, loss=0.019] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019334125581385052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 17702/17702 [07:18<00:00, 40.40batch/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01932401899170497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 17702/17702 [07:21<00:00, 40.05batch/s, loss=0.0189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01932016255622539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 17702/17702 [07:23<00:00, 39.92batch/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019317442841353106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating ...: 100%|██████████| 157/157 [00:04<00:00, 33.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. MSE: 3.710307282744907e-05 | Avg. MAE: 0.0062112160958349705\n",
      "current score: 0.0062112160958349705 | Last best score: 0.005997189786285162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 17702/17702 [07:18<00:00, 40.40batch/s, loss=0.019] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.01930878192147213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 17702/17702 [07:22<00:00, 40.03batch/s, loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019300562282661176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 17702/17702 [07:19<00:00, 40.24batch/s, loss=0.0197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 0.019295438417329173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49:  17%|█▋        | 2973/17702 [01:16<05:52, 41.81batch/s, loss=0.0193]"
     ]
    }
   ],
   "source": [
    "CLIP_Net = CLIP_Net.module if isinstance(CLIP_Net, torch.nn.DataParallel) else CLIP_Net\n",
    "train_ProbVLM(\n",
    "    CLIP_Net,\n",
    "    ProbVLM_Net,\n",
    "    coco_train_loader,\n",
    "    coco_valid_loader,\n",
    "    Cri = TempCombLoss(),\n",
    "    device='cuda:2',\n",
    "    dtype=torch.cuda.FloatTensor,\n",
    "    init_lr=8e-5,\n",
    "    num_epochs=500,\n",
    "    eval_every=5,\n",
    "    ckpt_path='exp/',\n",
    "    T1=1e0,\n",
    "    T2=1e-4,\n",
    "    save_name='coco_probVLM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneAgent(\n",
       "  (ProbVLM_Net): BayesCap_for_CLIP(\n",
       "    (img_BayesCap): BayesCap_MLP(\n",
       "      (mod): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (block_mu): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (block_alpha): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (block_beta): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (txt_BayesCap): BayesCap_MLP(\n",
       "      (mod): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (block_mu): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (block_alpha): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (block_beta): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (CLIP_Net): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (ClipCap): ClipCaptionPrefix(\n",
       "    (gpt): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "    (clip_project): TransformerMapper(\n",
       "      (transformer): Transformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x TransformerLayer(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiHeadAttention(\n",
       "              (to_queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (to_keys_values): Linear(in_features=768, out_features=1536, bias=False)\n",
       "              (project): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MlpTransformer(\n",
       "              (fc1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "              (fc2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=512, out_features=30720, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (GGL): GenGaussLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from one_agent import OneAgent\n",
    "agent = OneAgent(agent_name='A')\n",
    "agent = agent.cuda()\n",
    "agent.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "communication_field_setup\n",
      "finish communication_field_setup\n",
      "tensor([[49406,   320,   786,   593,   320,   736, 11122,   525,   320,  2442,\n",
      "           617,  2966,   525,   320, 11795,  1759,   269, 49407,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   786,  6765,   320,  7659,  3701,   525,   320, 11795,  1759,\n",
      "           525,   518, 16303,   269, 49407,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,   786,  6765,   525,   518,   893,   539,   320, 10297,\n",
      "           269, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320, 11795,  5035,   593,   320,  1888,  2533,   525,   320,\n",
      "          7659,  3701, 33775,   531,   518, 35186,   539,   320,   624, 28237,\n",
      "          2445,   593,   320,  2465,   537,   320,  5994,   539,  3887,   268,\n",
      "          3168, 47402,  5873,   269, 49407,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,   786,   530,   320,   736,  2523,   537,   320,   736,\n",
      "          3801,   533,   525,   320, 10297,   525,   320,  2682,  1145,   269,\n",
      "         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,  1888,  1611, 30786,   542,   593,   518, 18881,   539,\n",
      "         12087,   620,   320, 12674,   269, 49407,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,  1888,  1611,   533,  7365,   531, 10211,   620,   899,\n",
      "         12674,   269, 49407,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,  3551,   533,   531, 10211,   620,   518,  2688, 12674,\n",
      "           530,   320,  3814,   539,  1166, 10531,   269, 49407,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,  1611, 12087,   620,   518, 12674,   525,   550,   733,   268,\n",
      "          3867, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,  1274,  1611,   533,  1500,  1112,   531, 10211,   620,\n",
      "           320, 12674,   525,   320,  2442,  9753,   269, 49407,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')\n",
      "torch.Size([25000, 77])\n"
     ]
    }
   ],
   "source": [
    "agent.communication_field_setup(coco_test_loader, coco_test_loader)\n",
    "\n",
    "print(agent.w[:10])\n",
    "print(agent.w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  27%|██▋       | 212/782 [00:26<01:11,  7.95batch/s, loss=0.307]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb セル 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m CLIP_Net \u001b[39m=\u001b[39m CLIP_Net\u001b[39m.\u001b[39mmodule \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(CLIP_Net, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel) \u001b[39melse\u001b[39;00m CLIP_Net\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_ProbVLM(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     CLIP_Net,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     ProbVLM_Net,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     coco_test_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     coco_valid_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     Cri \u001b[39m=\u001b[39;49m TempCombLoss(),\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mFloatTensor,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     init_lr\u001b[39m=\u001b[39;49m\u001b[39m8e-5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     eval_every\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     ckpt_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodels/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     T1\u001b[39m=\u001b[39;49m\u001b[39m1e0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     T2\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39magent_A\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     w \u001b[39m=\u001b[39;49m agent\u001b[39m.\u001b[39;49mw\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f496e7465722d434c4950227d@ssh-remote%2Bdgx-station-a100/workspace/Inter-CLIP/Inter-ProbVLM/train_ProbVLM_CLIP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m )\n",
      "File \u001b[0;32m/workspace/Inter-CLIP/Inter-ProbVLM/ProbVLM/src/train_probVLM.py:73\u001b[0m, in \u001b[0;36mtrain_ProbVLM\u001b[0;34m(CLIP_Net, BayesCap_Net, train_loader, eval_loader, Cri, device, dtype, init_lr, num_epochs, eval_every, ckpt_path, cross_modal_lambda, T1, T2, save_name, w)\u001b[0m\n\u001b[1;32m     70\u001b[0m (img_mu, img_1alpha, img_beta), (txt_mu, txt_1alpha, txt_beta) \u001b[39m=\u001b[39m BayesCap_Net(xfI, xfT)\n\u001b[1;32m     72\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 73\u001b[0m loss_i \u001b[39m=\u001b[39m Cri(img_mu, img_1alpha, img_beta, xfI, T1\u001b[39m=\u001b[39;49mT1, T2\u001b[39m=\u001b[39;49mT2)\n\u001b[1;32m     74\u001b[0m loss_t \u001b[39m=\u001b[39m Cri(txt_mu, txt_1alpha, txt_beta, xfT, T1\u001b[39m=\u001b[39mT1, T2\u001b[39m=\u001b[39mT2)\n\u001b[1;32m     75\u001b[0m \u001b[39m#cross modal terms\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Inter-CLIP/Inter-ProbVLM/ProbVLM/src/losses.py:86\u001b[0m, in \u001b[0;36mTempCombLoss.forward\u001b[0;34m(self, mean, one_over_alpha, beta, target, T1, T2)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     81\u001b[0m \t\u001b[39mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m \tmean: Tensor, one_over_alpha: Tensor, beta: Tensor, target: Tensor,\n\u001b[1;32m     83\u001b[0m \tT1: \u001b[39mfloat\u001b[39m, T2: \u001b[39mfloat\u001b[39m\n\u001b[1;32m     84\u001b[0m ):\n\u001b[1;32m     85\u001b[0m \tl1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL_l1(mean, target)\n\u001b[0;32m---> 86\u001b[0m \tl2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mL_GenGauss(mean, one_over_alpha, beta, target)\n\u001b[1;32m     87\u001b[0m \tl \u001b[39m=\u001b[39m T1\u001b[39m*\u001b[39ml1 \u001b[39m+\u001b[39m T2\u001b[39m*\u001b[39ml2\n\u001b[1;32m     89\u001b[0m \t\u001b[39mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Inter-CLIP/Inter-ProbVLM/ProbVLM/src/losses.py:39\u001b[0m, in \u001b[0;36mGenGaussLoss.forward\u001b[0;34m(self, mean, one_over_alpha, beta, target)\u001b[0m\n\u001b[1;32m     36\u001b[0m log_beta \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(beta1)\n\u001b[1;32m     37\u001b[0m lgamma_beta \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlgamma(torch\u001b[39m.\u001b[39mpow(beta1, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 39\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39msum(log_one_over_alpha \u001b[39m!=\u001b[39m log_one_over_alpha) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m \t\u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlog_one_over_alpha has nan\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39msum(lgamma_beta \u001b[39m!=\u001b[39m lgamma_beta) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CLIP_Net = CLIP_Net.module if isinstance(CLIP_Net, torch.nn.DataParallel) else CLIP_Net\n",
    "train_ProbVLM(\n",
    "    CLIP_Net,\n",
    "    ProbVLM_Net,\n",
    "    coco_test_loader,\n",
    "    coco_valid_loader,\n",
    "    Cri = TempCombLoss(),\n",
    "    device='cuda',\n",
    "    dtype=torch.cuda.FloatTensor,\n",
    "    init_lr=8e-5,\n",
    "    num_epochs=500,\n",
    "    eval_every=5,\n",
    "    ckpt_path='models/',\n",
    "    T1=1e0,\n",
    "    T2=1e-4,\n",
    "    save_name='agent_A',\n",
    "    w = agent.w\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
