{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clip model and preprocess\n",
    "from clip import clip\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "\n",
    "# load coco_test dataset\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"dataset/dataset_cache/coco_test_dataset_5000.pkl\", \"rb\") as f:\n",
    "        test_dataset = pickle.load(f)\n",
    "\n",
    "# setting of matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from CLIP_prefix_caption.train import *\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "prefix_length = 10\n",
    "prefix_length_clip = 40\n",
    "prefix_dim = 512\n",
    "num_layers = 8\n",
    "\n",
    "ClipCap = ClipCaptionModel(prefix_length, prefix_length_clip, prefix_dim, num_layers,\"mlp\").to(device)\n",
    "ClipCap.load_state_dict(torch.load(\"models/official_model/clipcap_coco_weights.pt\"), strict=False)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# generate_test(ClipCap, CLIP_Net, dataloader, tokenizer, sample_num, generate_mode = \"beam\", device = \"cuda:0\", prefix_length = 10)\n",
    "for i in range(10):\n",
    "        generated_text = generate_test(ClipCap, clip_model, dataloader, tokenizer, 10, generate_mode = \"greedy\", device = \"cuda:0\", prefix_length = 10, temperature=0.7)\n",
    "        print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "with open(\"dataset/dataset_cache/coco_test_dataset_5000.pkl\", \"rb\") as f:\n",
    "    coco_test_dataset_A = pickle.load(f)\n",
    "    coco_test_dataset_A.prefix_length = 10\n",
    "\n",
    "for i in range(20,30):\n",
    "    image = coco_test_dataset_A.get_img(i)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"coco_{i}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting of matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from CLIP_prefix_caption.train import *\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "prefix_length = 10\n",
    "prefix_length_clip = 40\n",
    "prefix_dim = 512\n",
    "num_layers = 8\n",
    "\n",
    "ClipCap = ClipCaptionModel(prefix_length, prefix_length_clip, prefix_dim, num_layers,\"mlp\").to(device)\n",
    "ClipCap.load_state_dict(torch.load(\"models/official_model/clipcap_coco_weights.pt\"), strict=False)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# generate_test(ClipCap, CLIP_Net, dataloader, tokenizer, sample_num, generate_mode = \"beam\", device = \"cuda:0\", prefix_length = 10)\n",
    "generated_text = generate_test(ClipCap, clip_model, dataloader, tokenizer, 1, generate_mode = \"beam\", device = \"cuda:0\", prefix_length = 10)\n",
    "\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     img, caption, vlm_token, gpt_token, gpt_mask, index, image_id = batch\n",
    "#     img, gpt_token, gpt_mask = img.to(device), gpt_token.to(device), gpt_mask.to(device)\n",
    "#     prefix = clip_model.encode_image(img).to(device, dtype=torch.float32)\n",
    "#     prefix_embeds = ClipCap.clip_project(prefix).reshape(prefix.shape[0], prefix_length, -1)\n",
    "#     for idx, prefix_embed in enumerate(prefix_embeds):\n",
    "#         prefix_embed = prefix_embed.unsqueeze(0)    \n",
    "#         generated_text = generate_beam(ClipCap, tokenizer, embed=prefix_embed, temperature=0)[0]\n",
    "#         # generated_text = generate2(agent.ClipCap, agent.tokenizer, embed=prefix_embed, temperature=0.5)\n",
    "#         print(generated_text)\n",
    "#         print(caption[idx])\n",
    "#         plt.imshow(img[idx].cpu().permute(1, 2, 0))\n",
    "#         plt.show()\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "for i in range(10):\n",
    "    print(coco_train_dataset.gpt_tokens[i].shape)\n",
    "\n",
    "print(coco_train_dataset.max_seq_len)\n",
    "print(cc3m_train_dataset.max_seq_len)\n",
    "\n",
    "max_seq_len = max(coco_train_dataset.max_seq_len, cc3m_train_dataset.max_seq_len)\n",
    "coco_train_dataset.max_seq_len = max_seq_len\n",
    "cc3m_train_dataset.max_seq_len = max_seq_len\n",
    "\n",
    "coco_dataloader = DataLoader(coco_train_dataset, batch_size=64, shuffle=True)\n",
    "cc3m_dataloader = DataLoader(cc3m_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "coco_batch = next(iter(coco_dataloader))\n",
    "cc3m_batch = next(iter(cc3m_dataloader))\n",
    "\n",
    "print(coco_batch[0].shape, cc3m_batch[0].shape)\n",
    "print(len(coco_batch[1]), len(cc3m_batch[1]))\n",
    "print(coco_batch[2].shape, cc3m_batch[2].shape)\n",
    "print(coco_batch[3].shape, cc3m_batch[3].shape)\n",
    "print(coco_batch[4].shape, cc3m_batch[4].shape)\n",
    "print(coco_batch[5].shape, cc3m_batch[5].shape)\n",
    "\n",
    "\n",
    "# 二つのデータセットを結合\n",
    "concat_dataset = ConcatDataset([coco_train_dataset, cc3m_train_dataset])\n",
    "concat_dataset.prefix_length = coco_train_dataset.prefix_length\n",
    "print(len(concat_dataset))\n",
    "\n",
    "\n",
    "\n",
    "# 結合したデータセットのためのデータローダーを作成\n",
    "data_loader = DataLoader(concat_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# データローダーを使ってデータを取得するループの例\n",
    "for batch in data_loader:\n",
    "    # batchには二つのデータセットの要素が含まれる\n",
    "    # ここでデータの処理を行う\n",
    "    break\n",
    "\n",
    "print(batch[0].shape)\n",
    "\n",
    "# save the dataset\n",
    "import pickle\n",
    "with open(\"dataset/dataset_cache/coco_cc3m_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(concat_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/dataset_cache/coco_cc3m_train.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(batch[1][:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 40\n",
    "normalize_prefix = True\n",
    "dataset = ConceptualDatasetFull(data_mode=\"test\", transform=preprocess)\n",
    "\n",
    "print(dataset.max_seq_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 40\n",
    "normalize_prefix = True\n",
    "dataset = ConceptualDatasetFull(data_mode=\"train\", transform=preprocess)\n",
    "with open('dataset/dataset_cache/cc3m_train.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "from clip import clip\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 40\n",
    "normalize_prefix = True\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "dataset = ConceptualDatasetFull(data_mode=\"test\", transform=preprocess)\n",
    "print(len(dataset))\n",
    "with open(\"dataset/dataset_cache/conceptual_test_dataset_11467.pkl\", 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"dataset/dataset_cache/cc3m_valid.pkl\", \"rb\") as f:\n",
    "    valid_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"dataset/dataset_cache/cc3m_train.pkl\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset/'\n",
    "prefix_length = 40\n",
    "normalize_prefix = True\n",
    "dataset = ConceptualDatasetFull(data_mode=\"train\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from utils import *\n",
    "dataloader = DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "i = 0\n",
    "for batch in dataloader:\n",
    "    print(i, batch[1][0])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Dataset size:  50000\n",
      "Dataset size:  10000\n",
      "['A young woman is riding a surfboard in the ocean.', 'He is about to bite into his first piece of pizza.', 'A piece of paper that has the word love written all over it.', 'a living room with a couch, television and a window', 'a close up of a green window frame', 'A man taking a swing at a tennis ball', 'Two tennis players extend their arms to shake hands with their opponents.', 'Electronic devices are underneath a flat screen television.', 'A picture of the TV on top of a shelf.', 'A woman eating a pizza and smiling for the camera.']\n"
     ]
    }
   ],
   "source": [
    "from train_ClipCap import *\n",
    "from utils import *\n",
    "import pickle\n",
    "from clip import clip\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 10\n",
    "normalize_prefix = True\n",
    "dataset = CocoDataset(root = data_path, transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, datasize=\"50000\", data_mode=\"train\", use_imageid=True)\n",
    "print(\"Dataset size: \", len(dataset))\n",
    "\n",
    "# set dataset ids [10000, ..., 20000]\n",
    "dataset.ids = dataset.ids[10000:20000]\n",
    "dataset.vlm_tokens, dataset.gpt_tokens, dataset.captions = dataset.get_tokens_list()\n",
    "print(\"Dataset size: \", len(dataset))\n",
    "print(dataset.captions[:10])\n",
    "\n",
    "\n",
    "# save the dataset\n",
    "with open('dataset/dataset_cache/coco_train_dataset_10000_20000_imageid.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clip model and preprocess\n",
    "from clip import clip\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "from train_ClipCap import *\n",
    "from utils import *\n",
    "import pickle\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 10\n",
    "normalize_prefix = True\n",
    "dataset = CocoDataset(root = data_path, transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, data_mode=\"test\", datasize=\"10000\", use_imageid=True)\n",
    "print(\"Dataset size: \", len(dataset))\n",
    "\n",
    "# save the dataset\n",
    "with open(f'dataset/dataset_cache/coco_test_dataset_{len(dataset)}.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_ClipCap import *\n",
    "from utils import *\n",
    "import pickle\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 40\n",
    "normalize_prefix = True\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda:0\")\n",
    "# dataset = CocoDataset(root = data_path, transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, datasize=\"full_100\")\n",
    "# print(\"Dataset size: \", len(dataset))\n",
    "# # save the dataset\n",
    "# with open('dataset/dataset_cache/coco_train_dataset_100.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)\n",
    "\n",
    "\n",
    "test_dataset = CocoDataset(root = data_path, transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, data_mode=\"test\", datasize=\"full\", use_imageid=True)\n",
    "print(\"Dataset size: \", len(test_dataset))\n",
    "\n",
    "with open('dataset/dataset_cache/coco_test_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clip model and preprocess\n",
    "from clip import clip\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda:1\")\n",
    "\n",
    "datasize = 300\n",
    "\n",
    "from utils import *\n",
    "data_path = 'dataset/conceptual_captions_subset/'\n",
    "prefix_length = 40\n",
    "normalize_prefix = True\n",
    "# conceptual_dataset = ConceptualDatasetFull(data_mode=\"train\", transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, datasize=f\"full_{datasize}\")\n",
    "\n",
    "# with open(f'dataset/dataset_cache/conceptual_train_dataset_{datasize}.pkl', 'wb') as f:\n",
    "#     pickle.dump(conceptual_dataset, f)\n",
    "\n",
    "test_dataset = ConceptualDatasetFull(data_mode=\"test\", transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, datasize=f\"full_{datasize}\")\n",
    "print(\"Dataset size: \", len(test_dataset))\n",
    "\n",
    "with open(f'dataset/dataset_cache/conceptual_test_dataset_{len(test_dataset)}.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size: \", len(conceptual_dataset))\n",
    "# create the dataloader\n",
    "conceptual_dataloader = DataLoader(conceptual_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "batch = next(iter(conceptual_dataloader))\n",
    "print(batch[0].shape)\n",
    "\n",
    "# save the dataset\n",
    "with open('dataset/dataset_cache/conceptual_train_dataset_100.pkl', 'wb') as f:\n",
    "    pickle.dump(conceptual_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "# load clip model and preprocess\n",
    "from clip import clip\n",
    "\n",
    "_, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
    "\n",
    "from train_ClipCap import *\n",
    "from utils import *\n",
    "import pickle\n",
    "data_path = 'dataset/'\n",
    "prefix_length = 10\n",
    "normalize_prefix = True\n",
    "datasize = \"5000\"\n",
    "use_imageid = True\n",
    "dataset = CocoDataset(root = data_path,data_mode=\"test\", transform=preprocess, prefix_length=prefix_length, normalize_prefix=normalize_prefix, datasize=datasize, use_imageid=use_imageid)\n",
    "\n",
    "# save the dataset\n",
    "save_path = f'dataset/dataset_cache/coco_test_dataset_{datasize}'\n",
    "if use_imageid:\n",
    "    save_path += \"_imageid\"\n",
    "with open(f'{save_path}.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataloader.dataset.vlm_tokens)\n",
    "print(dataloader.dataset.vlm_tokens[[0,1,2,3,4,5,6,7,8,9]])\n",
    "vlm_tokens = torch.cat(dataloader.dataset.vlm_tokens, dim=0).reshape(len(dataloader.dataset), -1)\n",
    "print(vlm_tokens.shape)\n",
    "print(vlm_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ProbVLM.src.networks import *\n",
    "# from time import time\n",
    "# ProbVLM_Net = BayesCap_for_CLIP(inp_dim=512, out_dim=512, hid_dim=256, num_layers=3, p_drop=0.05,)\n",
    "# ProbVLM_Net.load_state_dict(torch.load(\"exp/debug/probvlm-022.pth\"))\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "# img = batch[0].cuda().float()\n",
    "# cap = batch[1]\n",
    "# vlm_token = batch[2]\n",
    "# gpt_token = batch[3]\n",
    "# print(vlm_token[0])\n",
    "# print(gpt_token[0])\n",
    "\n",
    "\n",
    "# print(cap[:2])\n",
    "\n",
    "# dataloader.dataset.caption_list[0] = \"yuki chan\"\n",
    "# dataloader.dataset.caption_list[1] = \"ayano chan\"\n",
    "\n",
    "# dataloader.dataset.gpt_tokens[0] = torch.tensor(dataloader.dataset.gpt_tokenizer.encode(\"yuki chan\"))\n",
    "# dataloader.dataset.gpt_tokens[1] = torch.tensor(dataloader.dataset.gpt_tokenizer.encode(\"ayano chan\"))\n",
    "# dataloader.dataset.vlm_tokens[0] = dataloader.dataset.vlm_tokenizer(\"yuki chan\")\n",
    "# dataloader.dataset.vlm_tokens[1] = dataloader.dataset.vlm_tokenizer(\"ayano chan\")\n",
    "\n",
    "\n",
    "# print(torch.tensor(dataloader.dataset.gpt_tokenizer.encode(\"yuki chan\")))\n",
    "# print(dataloader.dataset.vlm_tokenizer(\"yuki chan\"))\n",
    "\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "# img = batch[0].cuda().float()\n",
    "# cap = batch[1]\n",
    "# token = batch[2]\n",
    "\n",
    "# print(cap[:2])\n",
    "# print(token[:2])\n",
    "# print(batch[3][:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
